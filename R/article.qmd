---
title: "Using Geoparquet + R to dig up incredible (free) insights in the new USDA crop maps"
author: "Ketchbrook"
date: "Aug 4, 2023"
format: 
  html:
    toc: true
    toc-title: Outline
    theme:
      light: cosmo
      dark: darkly
    self-contained: true
    code-fold: show  
    highlight-style: nord
    code-link: true
editor: visual
---

![](images/hero_raw.png){fig-align="center"}

## A very thicc data drop from USDA

When the USDA dropped the Crop Sequence Boundaries (CSB) data set this summer, we knew it would be huge, but we didn't know what we should do with it off the bat. Much like a five year old who picking up a cool stick; we knew it was something, so we decided to start swinging it around our head until something sweet happened.

The [CSB](https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/) data set produces 'estimates of field boundaries, crop acreage, and crop rotations across the contiguous United States.' In a nutshell, it shows what is planted where, and how that has changed over time.

::: callout-note
## CSB: Crop Sequence Boundaries

For more see the USDA CSB homepage: [LINK](https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/index.php)
:::

[![CSB in action](images/1.PNG){fig-align="center"}](https://www.nass.usda.gov/Education_and_Outreach/Reports,_Presentations_and_Conferences/reports/conferences/ESRI-2023/Maximizing%20ArcGIS%20Pro%20for%20The%20Crop%20Sequence%20Boundaries%20(CSB)%20Project.pdf)

### Problem: it's large

After downloading the [geodatabase files](https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/) from the USDA CSB homepage, we quickly realized that we couldn't simply read these large files into our local R or python sessions. [QGIS](https://www.qgis.org/en/site/index.html) could handle the files, but we would much rather work in R.

### Initial thoughts

At this point, we knew we wanted to work with the data in R, but we couldn't even load the files into our local R session without crashing our not-weak PCs. So we drew up a plan:

1.  Get the data into a more efficient file format; read it in

2.  Come up with some good use cases for the CSB data

3.  Make some plots and maps that deliver useful insights

### The audience

As consultants, we always work backwards from what our customer needs/wants. In order to do that, we had to get into the head of someone in ag finance and think about what would be most useful within this new data set.

Crops are the primary output of (non-livestock) producers. Knowing what crops are planted where, and how the composition of those crops has changed over time (we believe) is useful for anyone marketing to folks whose business revolves around that type of thing.

So our audience is:

> Analysts, data scientists and BI folks working in the Farm Credit system.

## The analysis

With the task firmly established and our audience tacked down, we can begin the fun stuff.

### From geodatabase to geoparquet

We know that the geodatabase file(s) that our data comes in won't work for local analysis. The CSB homepage recommends using an ArcGIS to analyse the 19.5 million unique polygons within the data set - but we like open source and don't like the vendor tie-in and cost associated with using ArcGIS for this analysis.

[Geoparquet](https://geoparquet.org/) is a newer file type that leverages the powerful Apache parquet columnar data storage format, which stores data for efficient loads, compresses well and decreases data size tremendously.\* [\* [Intro to geoparquet](https://getindata.com/blog/introducing-geoparquet-data-format/).]{.aside}

The goal of the geoparquet format is become the new standard file format in the geospatial world, replacing the shapefile as the standard.

For our purposes, we want to be actually able to read those 19 million polygons into our R session, so we decided to convert the geodatabase file into partitioned geoparquet files.

In order to do this, we needed to saddle up and "cowboy code" some python to iteratively read in the data and write to parquet.

```{python}
#| eval: false

import geopandas as gpd

# Define path to the un-zipped .gdb
gdb_path = "./National_Final_gdb/CSB1522.gdb"

# Define path to destination folder to land .parquet files
parquet_dest = "./some/folder/somewhere/"

row_start = 0
row_end = 1000000

for r in range(0, 20):
    print(f'Writing rows {row_start}:{row_end}')
    rows = slice(row_start, row_end)
    geo = gpd.read_file(gdb_path, rows=rows)
    # If there's no data in the data frame, exit the loop (don't write to .parquet)
    if len(geo) == 0:
        break
    else:
        path = parquet_dest + "csb_" + str(r) + ".parquet"
        geo.to_parquet(path=path)
        row_start = row_end + 1
        row_end = (r + 2) * 1000000

```

We decided to partition the data by year and state. The most recent file contains an eight-year history of data, which is sufficient for our purposes now. By partitioning the data in such a way, we will reap huge performance gains on read.

The data was uploaded to a public Amazon s3 bucket that anyone can read from (including you).

```{r}
#| warning: false
#| message: false

library(arrow)
library(CGPfunctions)
library(dplyr)
library(ggcharts)
library(ggplot2)
library(gt)
library(here)
library(leaflet)
library(patchwork)
library(readr)
library(sfarrow)
library(tidyr)


bucket <- arrow::s3_bucket("ketchbrook-public-usda-nass-csb")
```

::: callout-tip
## You can use this dataset!

Ketchbrook Analytics has made this optimized geoparquet dataset available to the public.
:::

### But first, some shop talk

Now that we've established a connection to our optimized geoparquet files stored in the cloud, we should discuss best practices for working with these types of files.

First, we're gonna get lazy. This is a good thing. When we combine geoparquet (and the underlying Arrow guts) with [dplyr](https://dplyr.tidyverse.org/) verbs, we get *lazy evaluation*. Lazy evaluation simply means that we build a recipe of what we want to do with the data set, but we only apply the recipe when we are ready to bring the *transformed* data into memory. In other words, we can filter our data set by a single state without having to mess with all 47 others. \* [\* CSB omits Hawaii and Alaska.]{.aside}

For our purposes, we will analyze a small portion of the total United States, so this lazy evaluation will give us tremendous speed gainz. \* [\* [For more see our guest post on the Posit blog](https://posit.co/blog/shiny-and-arrow/).]{.aside}

### Light data cleaning

Let's take our first peek at the data to see what we're dealing with. Note that the [sfarrow](https://github.com/wcjochem/sfarrow) package let's us parse our geoparquet directly into the popular [sf](https://r-spatial.github.io/sf/) object.

```{r}

raw <- arrow::open_dataset(bucket) |>
  dplyr::filter(STATEFIPS == 9) |>  # Connecticut
  dplyr::filter(CNTYFIPS == "003") |>  # Hartford county
  sfarrow::read_sf_dataset()  # interpret `geometry` as sf object

dplyr::glimpse(raw)
```

So we a unique identifier for each field `CSBID`, we have what appear to be crop codes for a given year for that field `R15 â€¦ R22` and we have some fields that describe the polygon that we don't really care about now.

We need to find the crop code lookup data and join them into this dataframe.

Unfortunately, the crop codes are stored in an [ugly XML file](https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/metadata_Crop-Sequence-Boundaries-2022.htm), but take heart, for we have thrown the codes into a csv for easy parsing. \* [\* [See location URL](https://github.com).]{.aside}

```{r}
#| warning: false
#| message: false


lookup <- readr::read_csv(here::here("data/00_raw/crop_types.csv"))

gt::gt(head(lookup))
```

Now let's simply join this data in, after we clean up a bit.

```{r}
# remove any NAs
mydata <- raw |>
  dplyr::mutate(across(tidyselect::starts_with("R"), ~ tidyr::replace_na(as.numeric(.x), 0)))

# create long dataframe for joining
raw_long <- mydata |>
  dplyr::mutate(id = dplyr::row_number()) |>
  tidyr::pivot_longer(cols = tidyselect::starts_with("R"), names_to = "Column", values_to = "Value")

# join lookup table
result <- raw_long |>
  dplyr::left_join(lookup, by = c("Value" = "categorization_code")) |>
  sf::st_drop_geometry() |>
  tidyr::pivot_wider(id_cols = "id", names_from = "Column", values_from = "land_cover") |>
  dplyr::select(-id)


just_the_crops <- mydata |>
  dplyr::select(!tidyselect::starts_with("R"))

final <- dplyr::bind_cols(just_the_crops, result)

rename_function <- function(col_name) {
  dplyr::if_else(
    startsWith(col_name, "R"),
    return(paste0("20", substring(col_name, 2))),
    return(col_name)
  )
}

# Rename columns starting with 'R'
to_plot <- final |>
  dplyr::rename_with(rename_function, tidyselect::starts_with("R"))
```

Our year - crop fields are now intelligible:

```{r}
to_plot |> 
  sf::st_drop_geometry() |> 
  dplyr::select(CSBID, `2017`:`2022`) |> 
  head() |> 
  gt::gt()
```

Now we're dangerous. First let's do some basic plotting to make sure it looks right.

### First plots

```{r}
to_plot |> 
  ggplot() +
    geom_sf(aes(fill = `2022`))
```

This looks good. We plotted all the crop fields found in Hartford County, Connecticut in 2022.

### Let's make an infographic

We want to make a bite-sized data graphic that we can reuse over different geographies. For example, we could make a few plots that we could apply to a county, state, or the entire country - something scalable in other words.

#### Leaflet map

[Leaflet](https://rstudio.github.io/leaflet/) maps are handy interactive viz that we can embed in any html document. But before we make the map, let's find out which crops are most prevalent in Hartford county so we don't end up with a mess of a map.

```{r}
to_plot |> 
  sf::st_drop_geometry() |> 
  dplyr::group_by(`2022`) |> 
  dplyr::summarise(cnt = dplyr::n()) |> 
  dplyr::ungroup() |> 
  dplyr::arrange(dplyr::desc(cnt)) |> 
  dplyr::slice_head(n = 3) |> 
  gt::gt()
```

The top three types of crop are above. Let's filter our dataframe to include only those, and eliminate extra fields we won't need now.

```{r}
to_plot_flt <- to_plot |> 
  dplyr::filter(
    `2022` %in% c("Corn", "Grassland/Pasture", "Other Hay/Non Alfalfa")
  ) |> 
  dplyr::select(CSBID, `2022`, Shape_Area)
```

We should mutate the `Shape_Area` field from square meters to square acres at this point.

```{r}
to_plot_flt$Shape_Area <- round(to_plot_flt$Shape_Area / 4046.85642)
```

Then we need to go from a **long** to a **wide** dataframe format.

```{r}
to_plot_flt_wide <- to_plot_flt |> 
  tidyr::pivot_wider(
    names_from = "2022",
    values_from = "Shape_Area"
  ) 
```

```{r}
to_plot_flt_wide <- to_plot_flt_wide |> 
  sf::st_transform(4326)  # ensure we have consistent CRS
```

```{r}

# Create separate dataframes for our map
to_plot_flt_wide_hay <- to_plot_flt_wide |> 
  dplyr::filter(`Other Hay/Non Alfalfa` > 0)

to_plot_flt_wide_grass <- to_plot_flt_wide |> 
  dplyr::filter(`Grassland/Pasture` > 0)

to_plot_flt_wide_corn <- to_plot_flt_wide |> 
  dplyr::filter(Corn > 0)

# Create palettes for each map
pal.hay <- colorNumeric(
  palette = "viridis",
  domain = to_plot_flt_wide_hay$`Other Hay/Non Alfalfa`
)

pal.grass <- colorNumeric(
  palette = "viridis",
  domain = to_plot_flt_wide_grass$`Grassland/Pasture`
)

pal.corn <- colorNumeric(
  palette = "viridis",
  domain = to_plot_flt_wide_corn$Corn
)

# Create html popups
popupHay <- paste0("Crop: ", "Other Hay/Non Alfalfa", "<br>",
                   "Acres: ", to_plot_flt_wide_hay$`Other Hay/Non Alfalfa`)

popupGrass <- paste0("Crop: ", "Grassland/Pasture", "<br>",
                     "Acres: ", to_plot_flt_wide_grass$`Grassland/Pasture`)

popupCorn <- paste0("Crop: ", "Corn", "<br>",
                    "Acres: ", to_plot_flt_wide_corn$Corn)

# Put all the pieces together
my_map  <- leaflet() |> 
  addProviderTiles(providers$Esri.WorldTopoMap) |> 
  addPolygons(data = to_plot_flt_wide_hay, 
              fillColor = ~pal.hay(`Other Hay/Non Alfalfa`), 
              color = "#b2aeae", # you need to use hex colors
              fillOpacity = 0.8, 
              weight = 1, 
              smoothFactor = 0.2,
              group = "Other Hay/Non Alfalfa",
              popup = popupHay) |> 
  addPolygons(data = to_plot_flt_wide_grass, 
              fillColor = ~pal.grass(`Grassland/Pasture`), 
              color = "#b2aeae", # you need to use hex colors
              fillOpacity = 0.8, 
              weight = 1, 
              smoothFactor = 0.2,
              group = "Grassland/Pasture",
              popup = popupGrass) |> 
  addPolygons(data = to_plot_flt_wide_corn, 
              fillColor = ~pal.corn(Corn), 
              color = "#b2aeae", # you need to use hex colors
              fillOpacity = 0.8, 
              weight = 1, 
              smoothFactor = 0.2,
              group = "Corn",
              popup = popupCorn) |> 
 
  addLayersControl(
    overlayGroups = c("Other Hay/Non Alfalfa","Grassland/Pasture", "Corn"),
    options = layersControlOptions(collapsed = FALSE)
  ) |> 
  hideGroup(c("Grassland/Pasture", "Corn"))  |> 
  showGroup(c("Other Hay/Non Alfalfa")) |> 
  setView(lng = -72.552, lat = 41.992, zoom = 11)

```

::: column-page
```{r}

my_map
```
:::

This is interesting. Apparently they named a small brook after our consulting firm:

![](images/2.PNG){fig-align="center"}

#### Bar charts

Now let's summarize the top crops for the last four years in the data set. The [ggcharts](https://github.com/thomas-neitmann/ggcharts) package provides nice shortcuts to making faceted bar charts that we might as well use.

```{r}
my_bars <- to_plot |> 
  sf::st_drop_geometry() |> 
  tidyr::pivot_longer(
    cols = starts_with("2"),
    names_to = "crop_year",
    values_to = "crop"
      ) |> 
  dplyr::filter(crop_year >= 2019) |> 
  dplyr::group_by(crop_year, crop) |> 
  dplyr::summarise(`Total Acres` = sum(Shape_Area)) |>
  dplyr::arrange(dplyr::desc(`Total Acres`)) |> 
  dplyr::ungroup() |>
  
  ggcharts::bar_chart(
    x = crop,
    y = `Total Acres`,
    facet = crop_year,
    top_n = 5
  ) +
  scale_y_continuous(labels = scales::label_number(suffix = "M", scale = 1e-6)) +
  xlab("") +
  theme_minimal()
  

my_bars
```

#### Slope graph

Another way to visualize these same data points is with a slope graph, which more so focuses on how the different crop totals change year-over-year.

```{r}
my_slope <- to_plot |> 
  sf::st_drop_geometry() |> 
  tidyr::pivot_longer(
    cols = starts_with("2"),
    names_to = "crop_year",
    values_to = "crop"
      ) |> 
  dplyr::filter(crop_year >= 2019) |> 
  dplyr::group_by(crop_year, crop) |> 
  dplyr::summarise(Total_Acres = sum(Shape_Area)) |>
  dplyr::arrange(dplyr::desc(Total_Acres)) |> 
  dplyr::slice_head(n = 5) |> 
  dplyr::ungroup() |> 
  dplyr::mutate(Total_Acres = round(Total_Acres/1000000)) |> 
  
  newggslopegraph(
    Times = crop_year,
    Measurement = Total_Acres,
    Grouping = crop,
    Title = "Hartford County - Top Crops Over Time",
    SubTitle = "In Millions of Acres",
    Caption = "Source: USDA CSB"
  )
```

#### Patching them together

Now we need something to patch some of these charts together into our "infographic", but really, we're just composing some charts in a way that makes a pleasing layout that we can repeat ad nauseum.

Enter, the [patchwork](https://patchwork.data-imaginist.com/) package.

All we have to do is use this shorthand `/` to say we want these two charts stacked on top of each other.

```{r}
#| echo: false
my_map
my_slope / my_bars
```

Now we can easily compose all three charts into one infographic that we can share. Simple.

## What's next?

### Use our geoparquet!

We encourage you to use our public geoparquet files of this CSB data set. All you need to do is reference our s3 bucket like so:

```{r}
#| eval: false

bucket <- arrow::s3_bucket("ketchbrook-public-usda-nass-csb")
```

before using the framework in this article to query that s3 source.

### Enrich the data

There is a multitude of sources we can pick from to enrich this already-great data set. One that comes to mind first is the [crop progress and condition data](https://www.nass.usda.gov/Research_and_Science/Crop_Progress_Gridded_Layers/index.php) from the USDA. This is a raster data set that is updated throughout the growing season and would provide a valuable look into how well the current crops are progressing.

![Corn progress pulled up in QGIS](images/3.PNG){fig-align="center"}

Another source that springs to mind is joining known farm properties with the CSB data. Imagine you have a geospatial file with all the fields a producer farms. A simple geospatial join with our CSB data would provide a wealth of information on what that producer is doing that year.

One more useful source of data is the [USDA NASS Quickstats](https://quickstats.nass.usda.gov/) database, which lets you query survey and census data related to agriculture. If you'd like to query this data from R, the great [tidyUSDA](https://bradlindblad.github.io/tidyUSDA/) R package allows you to do just that, in addition to having built-in plotting functionality:

![Plotting with tidyUSDA R package](images/4.PNG){fig-align="center"}

Thanks for reading!
